{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "included-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statewide-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "armed-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim  // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        # self.n_heads, self.out_dim_per_head\n",
    "        # specify each dimensions\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "juvenile-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quality-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nuclear-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # autograd - false\n",
    "        # https://pytorch.org/docs/stable/notes/autograd.html\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] # x.size(1) = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "refined-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "introductory-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "multiple-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n"
     ]
    }
   ],
   "source": [
    "print('initializing..')\n",
    "batch_size = 128\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':2e-3, 'weight_decay':1e-4, 'betas':(.9,.999)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affected-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'europarl30k.fr.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "# https://docs.python.org/ja/3.9/library/re.html#re.sub\n",
    "# \\g<0>\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    # counter - most_common \n",
    "    # https://docs.python.org/ja/3/library/collections.html#counter-objects\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acknowledged-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n"
     ]
    }
   ],
   "source": [
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "#model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "emotional-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing optimizer and loss...\n"
     ]
    }
   ],
   "source": [
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "print_each = 5\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 30000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    #masked_input = masked_input.cuda(non_blocking=True)\n",
    "    #masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-pulse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
