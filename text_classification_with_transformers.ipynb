{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-546e4f5942fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-27 19:44:30--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "www.rondhuit.com (www.rondhuit.com) をDNSに問いあわせています... 59.106.19.174\n",
      "www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 8855190 (8.4M) [application/x-gzip]\n",
      "`ldcc-20140209.tar.gz.1' に保存中\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M   114KB/s 時間 72s        \n",
      "\n",
      "2021-03-27 19:45:44 (119 KB/s) - `ldcc-20140209.tar.gz.1' へ保存完了 [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!tar zxf ldcc-20140209.tar.gz > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"sports-watch\", \"topic-news\", \"dokujo-tsushin\", \"peachy\",\n",
    "    \"movie-enter\", \"kaden-channel\", \"livedoor-homme\", \"smax\",\n",
    "    \"it-life-hack\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for category in categories:\n",
    "    for f in glob.glob(f\"./text/{category}/{category}*.txt\"):\n",
    "        # 1ファイルごとに処理\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fin:\n",
    "            # nextで1行取得する(__next__を呼ぶ)\n",
    "            url = next(fin).strip()\n",
    "            date = next(fin).strip()\n",
    "            title = next(fin).strip()\n",
    "            body = \"\\n\".join([line.strip() for line in fin if line.strip()])\n",
    "      \n",
    "        docs.append((category, url, date, title, body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        docs,\n",
    "        columns=[\"category\", \"url\", \"date\", \"title\", \"body\"],\n",
    "        dtype=\"category\"\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "df = df.assign(\n",
    "    label=lambda df: pd.Series(le.fit_transform(df.category))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val = train_test_split(idx, random_state=123)\n",
    "df_train = df.loc[idx_train, ['body', 'label']]\n",
    "df_val = df.loc[idx_val, ['body', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_no_wakati = {\n",
    "    \"train\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_train['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_train),\n",
    "        'label': df_train['label'].tolist()\n",
    "    }),\n",
    "    \"validation\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_val['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_val),\n",
    "        'label': df_val['label'].tolist()\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map_fn(tokenizer, max_length=100):\n",
    "    \"\"\"map function for pretrained tokenizer\"\"\"\n",
    "    def _tokenize(text_a, text_b, label):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text_a.numpy().decode('utf-8'),\n",
    "            text_b.numpy().decode('utf-8'),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def _map_fn(data):\n",
    "        text_a = data['sentence1']\n",
    "        text_b = data['sentence2']\n",
    "        label = data['label']\n",
    "        out = tf.py_function(_tokenize, inp=[text_a, text_b, label], Tout=(tf.int32, tf.int32, tf.int32, tf.int32))\n",
    "        return (\n",
    "            {\"input_ids\": out[0], \"token_type_ids\": out[1], \"attention_mask\": out[2]},\n",
    "            out[3]\n",
    "        )\n",
    "    return _map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, tokenizer, model from pretrained vocabulary\n",
    "\n",
    "def load_dataset(data, tokenizer, max_length=128, train_batch=8, val_batch=32):\n",
    "    # Prepare dataset for BERT as a tf.data.Dataset instance\n",
    "    train_dataset = data['train'].map(tokenize_map_fn(tokenizer, max_length=max_length))\n",
    "    valid_dataset = data['validation'].map(tokenize_map_fn(tokenizer, max_length=max_length))\n",
    "    train_dataset = train_dataset.shuffle(100).padded_batch(train_batch, padded_shapes=({'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length}, []), drop_remainder=True)\n",
    "    valid_dataset = valid_dataset.padded_batch(val_batch, padded_shapes=({'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length}, []), drop_remainder=True)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[    2, 19300, 14230, ...,    51,     3,     3],\n",
      "       [    2,  1758,     5, ...,  1037,     3,     3],\n",
      "       [    2,  6338,     6, ..., 14071,     3,     3],\n",
      "       ...,\n",
      "       [    2,  2292,    12, ...,  5858,     3,     3],\n",
      "       [    2,    36, 20617, ...,     5,     3,     3],\n",
      "       [    2,  1789,  3057, ..., 16422,     3,     3]])>, 'token_type_ids': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1]])>, 'attention_mask': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]])>}, <tf.Tensor: shape=(8,), dtype=int32, numpy=array([2, 8, 4, 2, 2, 1, 3, 6])>)\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "\n",
    "data = {\n",
    "    \"train\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_train['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_train),\n",
    "        'label': df_train['label'].tolist()\n",
    "    }),\n",
    "    \"validation\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_val['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_val),\n",
    "        'label': df_val['label'].tolist()\n",
    "    })\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "#OSError: Model name 'bert-base-japanese' was not found in tokenizers model name list \n",
    "#(bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, \n",
    "#bert-base-japanese-char-whole-word-masking). We assumed 'bert-base-japanese' was a path, a model identifier, \n",
    "#or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
    "\n",
    "#https://github.com/huggingface/transformers/issues/4060\n",
    "#!pip install git+git://github.com/huggingface/transformers.git\n",
    "\n",
    "#train_dataset, valid_dataset = load_dataset(data)\n",
    "train_dataset, valid_dataset = load_dataset(data, tokenizer)\n",
    "\n",
    "#https://www.haya-programming.com/entry/2018/10/05/233418\n",
    "#TypeError: load_dataset() missing 1 required positional argument: 'tokenizer'\n",
    "\n",
    "\n",
    "for data in train_dataset.take(1):\n",
    "    print(data)\n",
    "\n",
    "#!pip install pip3 install --upgrade tensorflow \n",
    "#!pip install ipadic\n",
    "#!pip install ipywidgets\n",
    "#!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317e576898d84b9cbacdf6d4229f2438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=479.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff8c78f12544b7892da30111c18af36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=545149952.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x21115d31488>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(bert, num_classes, max_length, bert_frozen=True):\n",
    "    # use only first layer and froze it\n",
    "    bert.layers[0].trainable = not bert_frozen\n",
    "\n",
    "    # input\n",
    "    input_ids = Input(shape=(max_length, ), dtype='int32', name='input_ids')\n",
    "    attention_mask = Input(shape=(max_length, ), dtype='int32', name='attention_mask')\n",
    "    token_type_ids = Input(shape=(max_length, ), dtype='int32', name='token_type_ids')\n",
    "    inputs = [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "    # bert\n",
    "    x = bert.layers[0](inputs)\n",
    "    # x: sequence_output, pooled_output\n",
    "\n",
    "    # only use pooled_output\n",
    "    out = x[1]\n",
    "\n",
    "    # fc layer(add layers for transfer learning)\n",
    "    out = Dropout(0.25)(out)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(num_classes, activation='softmax')(out)\n",
    "    return Model(inputs=inputs, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 9)\n",
      "[0.05616339 0.09231102 0.12533331 0.12583362 0.16613422 0.11072103\n",
      " 0.09673732 0.04189603 0.18487014]\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "num_classes = 9\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "\n",
    "pred = model.predict(next(iter(train_dataset))[0])\n",
    "print(pred.shape)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
    "optimizer = optimizers.Adam()\n",
    "loss = losses.SparseCategoricalCrossentropy()\n",
    "metric = metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate acc by model:  0.0\n",
      "evaluate acc by hand:  0.0\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataset))\n",
    "evl = model.evaluate(sample[0], sample[1], verbose=0)\n",
    "print('evaluate acc by model: ', evl[1])\n",
    "\n",
    "import numpy as np\n",
    "pred = model.predict(sample[0])\n",
    "labels_np = sample[1].numpy()\n",
    "acc = np.sum((np.argmax(pred, axis=-1) == labels_np).astype(np.int64)) / labels_np.shape[0]\n",
    "print('evaluate acc by hand: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d143461abf3463389d8437c944468aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=257706.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad18e9959b57476ca34aa0981f0779bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=104.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/172 [..............................] - ETA: 1:01:50 - loss: 2.7798 - accuracy: 0.0955"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4071a3df3915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# # Train and evaluate using tf.keras.Model.fit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m history = model.fit(train_dataset, epochs=epochs,\n\u001b[1;32m---> 24\u001b[1;33m                     validation_data=valid_dataset)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---- bert-base-japanese ----\n",
    "epochs = 7\n",
    "max_length = 500\n",
    "train_batch = 32\n",
    "val_batch = 64\n",
    "num_classes = 9\n",
    "\n",
    "# Load dataset, tokenizer, model from pretrained vocabulary\n",
    "#tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "train_dataset, valid_dataset = load_dataset(data_no_wakati, tokenizer, max_length=max_length, train_batch=train_batch, val_batch=val_batch)\n",
    "\n",
    "# define fine-tuning model\n",
    "bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "#bert = TFBertModel.from_pretrained('bert-base-japanese')\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "optimizer = optimizers.Adam()\n",
    "loss = losses.SparseCategoricalCrossentropy()\n",
    "metric = metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# # Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "                    validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
