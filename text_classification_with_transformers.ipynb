{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "#internal error in cuda 11.2\n",
    "#https://stackoverflow.com/questions/65608713/tensorflow-gpu-could-not-load-dynamic-library-cusolver64-10-dll-dlerror-cuso\n",
    "#Rename file cusolver64_11.dll  To  cusolver64_10.dll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 459209408324930538,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6489348512\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3806864992512218152\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n",
      "指定されたパスが見つかりません。\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!tar zxf ldcc-20140209.tar.gz > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"sports-watch\", \"topic-news\", \"dokujo-tsushin\", \"peachy\",\n",
    "    \"movie-enter\", \"kaden-channel\", \"livedoor-homme\", \"smax\",\n",
    "    \"it-life-hack\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for category in categories:\n",
    "    for f in glob.glob(f\"./text/{category}/{category}*.txt\"):\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fin:\n",
    "            url = next(fin).strip()\n",
    "            date = next(fin).strip()\n",
    "            title = next(fin).strip()\n",
    "            body = \"\\n\".join([line.strip() for line in fin if line.strip()])\n",
    "      \n",
    "        docs.append((category, url, date, title, body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        docs,\n",
    "        columns=[\"category\", \"url\", \"date\", \"title\", \"body\"],\n",
    "        dtype=\"category\"\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "df = df.assign(\n",
    "    label=lambda df: pd.Series(le.fit_transform(df.category))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val = train_test_split(idx, random_state=123)\n",
    "df_train = df.loc[idx_train, ['body', 'label']]\n",
    "df_val = df.loc[idx_val, ['body', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_no_wakati = {\n",
    "    \"train\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_train['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_train),\n",
    "        'label': df_train['label'].tolist()\n",
    "    }),\n",
    "    \"validation\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_val['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_val),\n",
    "        'label': df_val['label'].tolist()\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map_fn(tokenizer, max_length=100):\n",
    "    \"\"\"map function for pretrained tokenizer\"\"\"\n",
    "    def _tokenize(text_a, text_b, label):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text_a.numpy().decode('utf-8'),\n",
    "            text_b.numpy().decode('utf-8'),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def _map_fn(data):\n",
    "        text_a = data['sentence1']\n",
    "        text_b = data['sentence2']\n",
    "        label = data['label']\n",
    "        out = tf.py_function(_tokenize, inp=[text_a, text_b, label], Tout=(tf.int32, tf.int32, tf.int32, tf.int32))\n",
    "        return (\n",
    "            {\"input_ids\": out[0], \"token_type_ids\": out[1], \"attention_mask\": out[2]},\n",
    "            out[3]\n",
    "        )\n",
    "    return _map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, tokenizer, model from pretrained vocabulary\n",
    "\n",
    "def load_dataset(data, tokenizer, max_length=128, train_batch=8, val_batch=32):\n",
    "    # Prepare dataset for BERT as a tf.data.Dataset instance\n",
    "    train_dataset = data['train'].map(tokenize_map_fn(tokenizer, max_length=max_length))\n",
    "    valid_dataset = data['validation'].map(tokenize_map_fn(tokenizer, max_length=max_length))\n",
    "    train_dataset = train_dataset.shuffle(100).padded_batch(train_batch, padded_shapes=({'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length}, []), drop_remainder=True)\n",
    "    valid_dataset = valid_dataset.padded_batch(val_batch, padded_shapes=({'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length}, []), drop_remainder=True)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[    2,   478, 29323, ...,     9,     3,     3],\n",
      "       [    2,  1387,  1107, ...,  6675,     3,     3],\n",
      "       [    2,  6338,     6, ..., 14071,     3,     3],\n",
      "       ...,\n",
      "       [    2,  1803,    40, ..., 10389,     3,     3],\n",
      "       [    2, 12453,    14, ...,    14,     3,     3],\n",
      "       [    2,  2617,  2184, ...,   482,     3,     3]])>, 'token_type_ids': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       [0, 0, 0, ..., 0, 0, 1]])>, 'attention_mask': <tf.Tensor: shape=(8, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]])>}, <tf.Tensor: shape=(8,), dtype=int32, numpy=array([1, 0, 4, 6, 0, 4, 6, 5])>)\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "\n",
    "data = {\n",
    "    \"train\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_train['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_train),\n",
    "        'label': df_train['label'].tolist()\n",
    "    }),\n",
    "    \"validation\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_val['body'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_val),\n",
    "        'label': df_val['label'].tolist()\n",
    "    })\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "#OSError: Model name 'bert-base-japanese' was not found in tokenizers model name list \n",
    "#(bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, \n",
    "#bert-base-japanese-char-whole-word-masking). We assumed 'bert-base-japanese' was a path, a model identifier, \n",
    "#or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
    "\n",
    "#https://github.com/huggingface/transformers/issues/4060\n",
    "#!pip install git+git://github.com/huggingface/transformers.git\n",
    "\n",
    "#train_dataset, valid_dataset = load_dataset(data)\n",
    "train_dataset, valid_dataset = load_dataset(data, tokenizer)\n",
    "\n",
    "#https://www.haya-programming.com/entry/2018/10/05/233418\n",
    "#TypeError: load_dataset() missing 1 required positional argument: 'tokenizer'\n",
    "\n",
    "\n",
    "for data in train_dataset.take(1):\n",
    "    print(data)\n",
    "\n",
    "#!pip install pip3 install --upgrade tensorflow \n",
    "#!pip install ipadic\n",
    "#!pip install ipywidgets\n",
    "#!pip install fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x25a349de348>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(bert, num_classes, max_length, bert_frozen=True):\n",
    "    # use only first layer and froze it\n",
    "    bert.layers[0].trainable = not bert_frozen\n",
    "\n",
    "    # input\n",
    "    input_ids = Input(shape=(max_length, ), dtype='int32', name='input_ids')\n",
    "    attention_mask = Input(shape=(max_length, ), dtype='int32', name='attention_mask')\n",
    "    token_type_ids = Input(shape=(max_length, ), dtype='int32', name='token_type_ids')\n",
    "    inputs = [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "    # bert\n",
    "    x = bert.layers[0](inputs)\n",
    "    # x: sequence_output, pooled_output\n",
    "\n",
    "    # only use pooled_output\n",
    "    out = x[1]\n",
    "\n",
    "    # fc layer(add layers for transfer learning)\n",
    "    out = Dropout(0.25)(out)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(num_classes, activation='softmax')(out)\n",
    "    return Model(inputs=inputs, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 9)\n",
      "[0.03915052 0.2689642  0.10826484 0.15069859 0.02105294 0.1548128\n",
      " 0.05509164 0.12900665 0.0729578 ]\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "num_classes = 9\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "\n",
    "pred = model.predict(next(iter(train_dataset))[0])\n",
    "print(pred.shape)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
    "optimizer = optimizers.Adam()\n",
    "loss = losses.SparseCategoricalCrossentropy()\n",
    "metric = metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate acc by model:  0.0\n",
      "evaluate acc by hand:  0.0\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataset))\n",
    "evl = model.evaluate(sample[0], sample[1], verbose=0)\n",
    "print('evaluate acc by model: ', evl[1])\n",
    "\n",
    "import numpy as np\n",
    "pred = model.predict(sample[0])\n",
    "labels_np = sample[1].numpy()\n",
    "acc = np.sum((np.argmax(pred, axis=-1) == labels_np).astype(np.int64)) / labels_np.shape[0]\n",
    "print('evaluate acc by hand: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 180s 509ms/step - loss: 1.9788 - accuracy: 0.3190 - val_loss: 1.1227 - val_accuracy: 0.6376\n",
      "Epoch 2/7\n",
      "345/345 [==============================] - 171s 494ms/step - loss: 1.3232 - accuracy: 0.5493 - val_loss: 0.9816 - val_accuracy: 0.6771\n",
      "Epoch 3/7\n",
      "345/345 [==============================] - 161s 465ms/step - loss: 1.1964 - accuracy: 0.5878 - val_loss: 0.8715 - val_accuracy: 0.7072\n",
      "Epoch 4/7\n",
      "345/345 [==============================] - 161s 466ms/step - loss: 1.1207 - accuracy: 0.6180 - val_loss: 0.8697 - val_accuracy: 0.7023\n",
      "Epoch 5/7\n",
      "345/345 [==============================] - 169s 488ms/step - loss: 1.0667 - accuracy: 0.6387 - val_loss: 0.7913 - val_accuracy: 0.7473\n",
      "Epoch 6/7\n",
      "345/345 [==============================] - 163s 471ms/step - loss: 1.0335 - accuracy: 0.6472 - val_loss: 0.7882 - val_accuracy: 0.7407\n",
      "Epoch 7/7\n",
      "345/345 [==============================] - 168s 486ms/step - loss: 1.0233 - accuracy: 0.6433 - val_loss: 0.7967 - val_accuracy: 0.7341\n"
     ]
    }
   ],
   "source": [
    "# ---- bert-base-japanese ----\n",
    "epochs = 7\n",
    "max_length = 500\n",
    "train_batch = 16\n",
    "val_batch = 32\n",
    "num_classes = 9\n",
    "\n",
    "# Load dataset, tokenizer, model from pretrained vocabulary\n",
    "#tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "train_dataset, valid_dataset = load_dataset(data_no_wakati, tokenizer, max_length=max_length, train_batch=train_batch, val_batch=val_batch)\n",
    "\n",
    "# define fine-tuning model\n",
    "bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "#bert = TFBertModel.from_pretrained('bert-base-japanese')\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "optimizer = optimizers.Adam()\n",
    "loss = losses.SparseCategoricalCrossentropy()\n",
    "metric = metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# # Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "                    validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
